# Kriechen
Kriechen is an experiment/"showcase", built specifically as an exercise with [asyncio](https://docs.python.org/3/library/asyncio.html). It exposes an configurable, fully asynchronous web crawler, built atop `asyncio.Queue`.

It exposes two main classes:
- [Crawler](./src/crawler.py), a generic task manager that orchestrates the execution of Consumers and Producers that feed back into each others' source queues
- [Spider](./src/spider.py), a class that uses the `Crawler` to create Producers that generate a queue of links to process, and Consumers that download the HTML of these links

The `Spider` is, essentially, a `Crawler` configured specifically to trawl the web. A `Crawler` can, in theory, be configured with different `Transformers` to allow it to be used for other tasks that involve exploring a hyperlinked graph structure, such as crawling a file system.

Because this is an experiment, I didn't implement a test suite. Instead, this repo is intended to demonstrate:
- The use of `asyncio`, viz.:
  - `asyncio.Queue`
  - Fully asynchronous (`put`/`get` without `nowait`), circular Producer-Consumer implementation
- An event-driven architecture for handling termination logic and shared data access
- Higher-Order Functions in the construction of the [Transformer](./src/transformer.py) used to configure the `Crawler` for specific purposes

[Documentation is available on Read the Docs](https://kriechen.readthedocs.io/en/latest/).

## Spider
### Usage
To use the `Spider`, instantiate with `url` and `max_links` arguments. The `url` will be the page from which to begin crawling, while `max_links` serves as the upper bound on the number of pages to request and process.

```python
from kriechen import Spider

spider = Spider(url="https://www.tagesschau.de", max_links=100)

# Crawl the site...
await spider.crawl()

# Extracts Text from Soup
result_text_only: List[str] = spider.results(extract_text=True)
```

The `spider.results` property refers to a `List` of [Page](./src/page.py) instances, each of which represents web page by encapsulating its raw aiohttp Response; raw HTML; and Beautiful Soup representation.

## Crawler
### Concept
The `Crawler` is more general entity, from which the `Spider` is derived. It works by creating a configurable number of Producers and Consumers, and two underlying queues, called Source and Sink.

Consumers pull elements from the Source queue, and write to the Sink. Producers pull elements from the Sink, and write to the Source. This allows the Consumer to:
- Retrieve an element from the Source queue
- Process the element
- Either transform the element or use the results of processing it to feed back to the Producer(s)

In the case of a web crawler, like `Spider`, the Consumer implements the above pattern with the following concrete steps:
- Retrieve a URL from the Source queue
- Fetch the contents of the page represented by the URL and create a `Page` instance
- Place a list of the internal links on the generated `Page` onto the Sink queue

The list of internal links placed by the Consumer onto the Sink queue gets "fanned out" by the Producer that consumes it — i.e., it takes the list off the queue, and places each one back onto the Source to be processed by a Consumer.

Consumers and Producers are each configured with a [Transformer](./src/transformer.py), a class whose instances contain references to functions used by the Producers and Consumers during processing:
- `fn_raw`: When a Producer/Consumer reads an element from the queue, it immediately invokes `fn_raw`, which transforms the element retrieved from the queue into an element that can be processed with business logic. This can be used to, for instance, convert a queue element containing a raw element and a priority into just the element to process (without the priority). In the case of `Spider`, `fn_raw` is a no-op — it simply returns its argument, becase the elements coming off of the queue are the URLs to process, and require no transformation.
- `fn`: This is the function that contains the processing "business logic". In the case of `Spider`, the Consumer's `fn` creates a `Page` with the provided URL.
-`fn_sink`: This function is invoked on the results of invoking `fn`, and is used to transform the processed element to be placed back into the feedback loop between Producers/Consumers. In the case of `Spider`, the `fn_sink` function for Consumers returns a list of the internal links on the `Page` instance generated by `fn`, which, in turn, are fanned back out to consumers by the Producers.

As Producers/Consumers process their workloads, Consumers places the results of their processing into a [Registry](./src/registry.py), which simply collates their results in a centralized location. Upon processing, a Producer/Consumer emits an [event](./src/events.py) — either `CrawlerEvents.Producer.PROCESSED_ITEM` or `CrawlerEvents.Consumer.PROCESSED_ITEM`, respectively. These events are detected by the parent `Crawler`, and trigger it to add the result of processing to its underlying registry.

Upon updating the registry, the underlying `Registry` emits an `CrawlerEvents.Registry.REGISTRY_UPDATED`. This event triggers the parent `Crawler`'s `watch` method, which checks the length of its underlying registry. If the registry is full, `watch` emits a `CrawlerEvents.Crawler.TERMINATE` event, which triggers the parent `Crawler`'s `Destructor` to initiate a shutdown.

Shutdown initiates the following sequence of events:
- Terminate all Producers and Consumers
- Drain any remaining elements on the Source and Sink queues — drained elements do _not_ get processed
- Flag the `Crawler` as `shutting_down` (enables internal consistency checks)

## Techniques Worth Mentioning
The codebase contains a number of techniques worth mentioning, viz.:
- Task Groups instead of `asyncio.gather`
- The use of Higher-Order Functions to configure the termination condition of the parent `Crawler` and allow users to customize the behavior of its Producers and Consumers
- A fully asynchronous "ring queue", in which Producers generate input to Consumers, and Consumers generate input to Producers
- An event-driven architecture for termiating the `Crawler` in a "responsive" fashion

My original intent was to write about each of these in detail, but I've decided to let the code stand for itself. If enough people ask enough questions, though, I'll write something up to answer them.